
import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
import re
import os
import time

def crawl_7eleven():
    start_ts = datetime.now()
    brand_name = "7-Eleven"
    print(f"ğŸš€ [{brand_name}] ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    all_products = []
    event_configs = [(1, "1+1"), (2, "2+1")]
    url = "https://www.7-eleven.co.kr/product/listMoreAjax.asp"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Referer": "https://www.7-eleven.co.kr/product/presentList.asp"
    }

    for p_tab, event_label in event_configs:
        print(f" ğŸ“¦ {event_label} ìƒí’ˆ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        payload = {"intPageSize": 10000, "pTab": p_tab, "currPage": 1}

        try:
            response = requests.post(url, headers=headers, data=payload)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                items = soup.select("li")
                for item in items:
                    try:
                        name_tag = item.select_one(".name")
                        if not name_tag: continue
                        name = name_tag.get_text(strip=True)
                        price_tag = item.select_one(".price span")
                        price = int(re.sub(r'[^0-9]', '', price_tag.get_text(strip=True).replace(',', ''))) if price_tag else 0
                        event_tag = item.select_one(".tag_list_01 li")
                        event = event_tag.get_text(strip=True) if event_tag else event_label
                        img_tag = item.select_one(".pic_product img")
                        img_url = f"https://www.7-eleven.co.kr{img_tag.get('src')}" if img_tag else ""
                        all_products.append({"brand": "7Eleven", "name": name, "price": price, "event": event, "img_url": img_url})
                    except Exception: continue
        except Exception as e:
            print(f" âŒ {event_label} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

    if all_products:
        df = pd.DataFrame(all_products)
        raw_count = len(df)
        df = df.drop_duplicates(subset=['name', 'event'], keep='first')
        
        today = datetime.now().strftime("%y%m%d")
        file_name = f"7Eleven_{today}.csv"
        os.makedirs("data", exist_ok=True)
        file_path = os.path.join("data", file_name)
        df.to_csv(file_path, index=False, encoding='utf-8-sig')

        duration = datetime.now() - start_ts
        print(f"\nìµœì¢… ê²°ê³¼ ìš”ì•½:")
        print(f" - ì „ì²´ ìˆ˜ì§‘ ê°œìˆ˜: {raw_count}")
        print(f" - ì¤‘ë³µ ì œê±° í›„  : {len(df)}")
        print(f" - ì €ì¥ íŒŒì¼ëª…   : {file_path}")
        print(f" - ì†Œìš” ì‹œê°„     : {duration.seconds // 60}ë¶„ {duration.seconds % 60}ì´ˆ")
    else:
        print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def scrape():
    crawl_7eleven()

if __name__ == "__main__":
    scrape()

import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
import re
import os
import time

def crawl_7eleven():
    start_ts = datetime.now()
    brand_name = "7-Eleven"
    print(f"ğŸš€ [{brand_name}] ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    all_products = []
    event_configs = [(1, "1+1"), (2, "2+1")]
    url = "https://www.7-eleven.co.kr/product/listMoreAjax.asp"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Referer": "https://www.7-eleven.co.kr/product/presentList.asp"
    }

    for p_tab, event_label in event_configs:
        print(f" ğŸ“¦ {event_label} ìƒí’ˆ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        payload = {"intPageSize": 10000, "pTab": p_tab, "currPage": 1}

        try:
            response = requests.post(url, headers=headers, data=payload)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                items = soup.select("li")
                for item in items:
                    try:
                        name_tag = item.select_one(".name")
                        if not name_tag: continue
                        name = name_tag.get_text(strip=True)
                        price_tag = item.select_one(".price span")
                        price = int(re.sub(r'[^0-9]', '', price_tag.get_text(strip=True).replace(',', ''))) if price_tag else 0
                        event_tag = item.select_one(".tag_list_01 li")
                        event = event_tag.get_text(strip=True) if event_tag else event_label
                        img_tag = item.select_one(".pic_product img")
                        img_url = f"https://www.7-eleven.co.kr{img_tag.get('src')}" if img_tag else ""
                        all_products.append({"brand": "7Eleven", "name": name, "price": price, "event": event, "img_url": img_url})
                    except Exception: continue
        except Exception as e:
            print(f" âŒ {event_label} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

    if all_products:
        df = pd.DataFrame(all_products)
        raw_count = len(df)
        df = df.drop_duplicates(subset=['name', 'event'], keep='first')
        
        today = datetime.now().strftime("%y%m%d")
        file_name = f"7Eleven_{today}.csv"
        os.makedirs("data", exist_ok=True)
        file_path = os.path.join("data", file_name)
        df.to_csv(file_path, index=False, encoding='utf-8-sig')

        duration = datetime.now() - start_ts
        print(f"\nìµœì¢… ê²°ê³¼ ìš”ì•½:")
        print(f" - ì „ì²´ ìˆ˜ì§‘ ê°œìˆ˜: {raw_count}")
        print(f" - ì¤‘ë³µ ì œê±° í›„  : {len(df)}")
        print(f" - ì €ì¥ íŒŒì¼ëª…   : {file_path}")
        print(f" - ì†Œìš” ì‹œê°„     : {duration.seconds // 60}ë¶„ {duration.seconds % 60}ì´ˆ")
    else:
        print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def scrape():
    crawl_7eleven()

if __name__ == "__main__":
    scrape()

